{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading files from huggingface/transformers examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/ model/ runs/ labels.txt requirements.txt run_ner.py tasks.py utils_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-09 14:41:37--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/train.conll\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125059 (122K) [text/plain]\n",
      "Saving to: ‘train.conll’\n",
      "\n",
      "train.conll         100%[===================>] 122.13K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2021-02-09 14:41:37 (1.49 MB/s) - ‘train.conll’ saved [125059/125059]\n",
      "\n",
      "--2021-02-09 14:41:38--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/dev.conll\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14109 (14K) [text/plain]\n",
      "Saving to: ‘dev.conll’\n",
      "\n",
      "dev.conll           100%[===================>]  13.78K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-02-09 14:41:38 (5.71 MB/s) - ‘dev.conll’ saved [14109/14109]\n",
      "\n",
      "--2021-02-09 14:41:38--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/test.conll\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34661 (34K) [text/plain]\n",
      "Saving to: ‘test.conll’\n",
      "\n",
      "test.conll          100%[===================>]  33.85K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-02-09 14:41:38 (2.84 MB/s) - ‘test.conll’ saved [34661/34661]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you need to download data\n",
    "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/train.conll\n",
    "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/dev.conll\n",
    "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/test.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Requirements\n",
    "!wget -q https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q conllu\n",
    "\n",
    "# Actual code\n",
    "!wget -q https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/run_ner.py\n",
    "!wget -q https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/utils_ner.py\n",
    "!wget -q https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/tasks.py   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cp train.conll data/train.txt\n",
    "!cp dev.conll data/dev.txt\n",
    "!cp test.conll data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/train.txt data/dev.txt data/test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-BOOK\n",
      "B-COMPOSER\n",
      "B-FILM\n",
      "B-SINGER\n",
      "B-SONG\n",
      "I-BOOK\n",
      "I-COMPOSER\n",
      "I-FILM\n",
      "I-SINGER\n",
      "I-SONG\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "!cat labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 17:12:01.781094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "usage: run_ner.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                  [--config_name CONFIG_NAME] [--task_type TASK_TYPE]\n",
      "                  [--tokenizer_name TOKENIZER_NAME] [--use_fast [USE_FAST]]\n",
      "                  [--cache_dir CACHE_DIR] --data_dir DATA_DIR\n",
      "                  [--labels LABELS] [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                  [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "                  [--output_dir OUTPUT_DIR]\n",
      "                  [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                  [--do_predict [DO_PREDICT]]\n",
      "                  [--evaluation_strategy {no,steps,epoch}]\n",
      "                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                  [--learning_rate LEARNING_RATE]\n",
      "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
      "                  [--max_grad_norm MAX_GRAD_NORM]\n",
      "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                  [--max_steps MAX_STEPS]\n",
      "                  [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                  [--warmup_steps WARMUP_STEPS] [--logging_dir LOGGING_DIR]\n",
      "                  [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                  [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
      "                  [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda [NO_CUDA]]\n",
      "                  [--seed SEED] [--fp16 [FP16]]\n",
      "                  [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                  [--fp16_backend {auto,amp,apex}] [--local_rank LOCAL_RANK]\n",
      "                  [--tpu_num_cores TPU_NUM_CORES]\n",
      "                  [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug [DEBUG]]\n",
      "                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                  [--eval_steps EVAL_STEPS]\n",
      "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
      "                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                  [--greater_is_better GREATER_IS_BETTER]\n",
      "                  [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                  [--sharded_ddp [SHARDED_DDP]] [--deepspeed DEEPSPEED]\n",
      "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                  [--adafactor [ADAFACTOR]]\n",
      "                  [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                  [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                  [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                  [--no_dataloader_pin_memory]\n",
      "                  [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        Path to pretrained model or model identifier from\n",
      "                        huggingface.co/models\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --task_type TASK_TYPE\n",
      "                        Task type to fine tune in training (e.g. NER, POS,\n",
      "                        etc)\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --use_fast [USE_FAST]\n",
      "                        Set this flag to use fast tokenization.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pretrained models\n",
      "                        downloaded from huggingface.co\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .txt files for\n",
      "                        a CoNLL-2003-formatted task.\n",
      "  --labels LABELS       Path to a file containing all labels. If not\n",
      "                        specified, CoNLL-2003 labels are used.\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated, sequences shorter will be padded.\n",
      "  --overwrite_cache [OVERWRITE_CACHE]\n",
      "                        Overwrite the cached training and evaluation sets\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
      "                        Overwrite the content of the output directory.Use this\n",
      "                        to continue training if output_dir points to a\n",
      "                        checkpoint directory.\n",
      "  --do_train [DO_TRAIN]\n",
      "                        Whether to run training.\n",
      "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
      "  --do_predict [DO_PREDICT]\n",
      "                        Whether to run predictions on the test set.\n",
      "  --evaluation_strategy {no,steps,epoch}\n",
      "                        The evaluation strategy to use.\n",
      "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
      "                        When performing evaluation and predictions, only\n",
      "                        returns the loss.\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for training.\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_train_batch_size`\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
      "                        training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
      "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
      "                        evaluation.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
      "                        Number of predictions steps to accumulate before\n",
      "                        moving the tensors to the CPU.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for AdamW.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay for AdamW if we apply some.\n",
      "  --adam_beta1 ADAM_BETA1\n",
      "                        Beta1 for AdamW optimizer\n",
      "  --adam_beta2 ADAM_BETA2\n",
      "                        Beta2 for AdamW optimizer\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for AdamW optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
      "                        The scheduler type to use.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --logging_dir LOGGING_DIR\n",
      "                        Tensorboard log dir.\n",
      "  --logging_first_step [LOGGING_FIRST_STEP]\n",
      "                        Log the first global_step\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\n",
      "                        Limit the total amount of checkpoints.Deletes the\n",
      "                        older checkpoints in the output_dir. Default is\n",
      "                        unlimited checkpoints\n",
      "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
      "  --seed SEED           Random seed that will be set at the beginning of\n",
      "                        training.\n",
      "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA Apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --fp16_backend {auto,amp,apex}\n",
      "                        The backend to be used for mixed precision.\n",
      "  --local_rank LOCAL_RANK\n",
      "                        For distributed training: local_rank\n",
      "  --tpu_num_cores TPU_NUM_CORES\n",
      "                        TPU: Number of TPU cores (automatically passed by\n",
      "                        launcher script)\n",
      "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
      "                        Deprecated, the use of `--debug` is preferred. TPU:\n",
      "                        Whether to print debug metrics\n",
      "  --debug [DEBUG]       Whether to print debug metrics on TPU\n",
      "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
      "                        Drop the last incomplete batch if it is not divisible\n",
      "                        by the batch size.\n",
      "  --eval_steps EVAL_STEPS\n",
      "                        Run an evaluation every X steps.\n",
      "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
      "                        Number of subprocesses to use for data loading\n",
      "                        (PyTorch only). 0 means that the data will be loaded\n",
      "                        in the main process.\n",
      "  --past_index PAST_INDEX\n",
      "                        If >=0, uses the corresponding part of the output as\n",
      "                        the past state for next step.\n",
      "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
      "                        wandb logging.\n",
      "  --disable_tqdm DISABLE_TQDM\n",
      "                        Whether or not to disable the tqdm progress bars.\n",
      "  --no_remove_unused_columns\n",
      "                        Remove columns not required by the model when using an\n",
      "                        nlp.Dataset.\n",
      "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
      "                        Remove columns not required by the model when using an\n",
      "                        nlp.Dataset.\n",
      "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
      "                        The list of keys in your dictionary of inputs that\n",
      "                        correspond to the labels.\n",
      "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
      "                        Whether or not to load the best model found during\n",
      "                        training at the end of training.\n",
      "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
      "                        The metric to use to compare two different models.\n",
      "  --greater_is_better GREATER_IS_BETTER\n",
      "                        Whether the `metric_for_best_model` should be\n",
      "                        maximized or not.\n",
      "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
      "                        When resuming training, whether or not to skip the\n",
      "                        first epochs and batches to get to the same training\n",
      "                        data.\n",
      "  --sharded_ddp [SHARDED_DDP]\n",
      "                        Whether or not to use sharded DDP training (in\n",
      "                        distributed training only).\n",
      "  --deepspeed DEEPSPEED\n",
      "                        Enable deepspeed and pass the path to deepspeed json\n",
      "                        config file (e.g. ds_config.json)\n",
      "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
      "                        The label smoothing epsilon to apply (zero means no\n",
      "                        label smoothing).\n",
      "  --adafactor [ADAFACTOR]\n",
      "                        Whether or not to replace AdamW by Adafactor.\n",
      "  --group_by_length [GROUP_BY_LENGTH]\n",
      "                        Whether or not to group samples of roughly the same\n",
      "                        length together when batching.\n",
      "  --report_to REPORT_TO [REPORT_TO ...]\n",
      "                        The list of integrations to report the results and\n",
      "                        logs to.\n",
      "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
      "                        When using distributed training, the value of the flag\n",
      "                        `find_unused_parameters` passed to\n",
      "                        `DistributedDataParallel`.\n",
      "  --no_dataloader_pin_memory\n",
      "                        Whether or not to pin memory for DataLoader.\n",
      "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
      "                        Whether or not to pin memory for DataLoader.\n"
     ]
    }
   ],
   "source": [
    "!python run_ner.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 17:13:23.818490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "02/09/2021 17:13:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "02/09/2021 17:13:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./model, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=20, logging_dir=runs/Feb09_17-13-24_a6555db87f3a, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=True, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=2)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 17:13:25,177 >> loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 17:13:25,177 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-BOOK\",\n",
      "    \"1\": \"B-COMPOSER\",\n",
      "    \"2\": \"B-FILM\",\n",
      "    \"3\": \"B-SINGER\",\n",
      "    \"4\": \"B-SONG\",\n",
      "    \"5\": \"I-BOOK\",\n",
      "    \"6\": \"I-COMPOSER\",\n",
      "    \"7\": \"I-FILM\",\n",
      "    \"8\": \"I-SINGER\",\n",
      "    \"9\": \"I-SONG\",\n",
      "    \"10\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"B-BOOK\": 0,\n",
      "    \"B-COMPOSER\": 1,\n",
      "    \"B-FILM\": 2,\n",
      "    \"B-SINGER\": 3,\n",
      "    \"B-SONG\": 4,\n",
      "    \"I-BOOK\": 5,\n",
      "    \"I-COMPOSER\": 6,\n",
      "    \"I-FILM\": 7,\n",
      "    \"I-SINGER\": 8,\n",
      "    \"I-SONG\": 9,\n",
      "    \"O\": 10\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.3.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 17:13:25,586 >> loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 17:13:25,587 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.3.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 17:13:26,097 >> loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 17:13:27,045 >> loading weights file https://huggingface.co/xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4b3ca85a63804fb7cd317765d9de19ce6208ee0fc9691b209384ee7cfd9cb3b9.64b4693d874c772310b8acda9a1193cfade77d56795a9b488e612f198b68f6f7\n",
      "[WARNING|modeling_utils.py:1134] 2021-02-09 17:13:36,973 >> Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1145] 2021-02-09 17:13:36,973 >> Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "02/09/2021 17:13:36 - INFO - filelock -   Lock 139632277307296 acquired on ./data/cached_train_XLMRobertaTokenizer_128.lock\n",
      "02/09/2021 17:13:36 - INFO - utils_ner -   Loading features from cached file ./data/cached_train_XLMRobertaTokenizer_128\n",
      "02/09/2021 17:13:37 - INFO - filelock -   Lock 139632277307296 released on ./data/cached_train_XLMRobertaTokenizer_128.lock\n",
      "02/09/2021 17:13:37 - INFO - filelock -   Lock 139632277307200 acquired on ./data/cached_dev_XLMRobertaTokenizer_128.lock\n",
      "02/09/2021 17:13:37 - INFO - utils_ner -   Loading features from cached file ./data/cached_dev_XLMRobertaTokenizer_128\n",
      "02/09/2021 17:13:37 - INFO - filelock -   Lock 139632277307200 released on ./data/cached_dev_XLMRobertaTokenizer_128.lock\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:702: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:837] 2021-02-09 17:13:39,492 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 17:13:39,492 >>   Num examples = 1966\n",
      "[INFO|trainer.py:839] 2021-02-09 17:13:39,492 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-09 17:13:39,492 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:841] 2021-02-09 17:13:39,492 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:842] 2021-02-09 17:13:39,492 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 17:13:39,492 >>   Total optimization steps = 155\n",
      "  0%|                                                   | 0/155 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
      " 20%|████████▍                                 | 31/155 [00:41<02:35,  1.26s/it][INFO|trainer.py:1600] 2021-02-09 17:14:21,253 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:14:21,254 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:14:21,254 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████▎                                     | 2/14 [00:00<00:02,  4.76it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 3/14 [00:00<00:03,  3.65it/s]\u001b[A\n",
      " 29%|████████████▌                               | 4/14 [00:01<00:03,  3.15it/s]\u001b[A\n",
      " 36%|███████████████▋                            | 5/14 [00:01<00:03,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 6/14 [00:02<00:02,  2.70it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 7/14 [00:02<00:02,  2.60it/s]\u001b[A\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:02<00:02,  2.53it/s]\u001b[A\n",
      " 64%|████████████████████████████▎               | 9/14 [00:03<00:02,  2.48it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:03<00:01,  2.45it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:04<00:01,  2.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:04<00:00,  2.41it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:05<00:00,  2.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.9092312455177307, 'eval_accuracy_score': 0.6579601990049752, 'eval_precision': 0.30194805194805197, 'eval_recall': 0.3661417322834646, 'eval_f1': 0.3309608540925267, 'eval_runtime': 6.2532, 'eval_samples_per_second': 35.022, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 31/155 [00:48<02:35,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.40it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 62/155 [01:27<01:57,  1.26s/it][INFO|trainer.py:1600] 2021-02-09 17:15:07,318 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:15:07,318 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:15:07,318 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████▎                                     | 2/14 [00:00<00:02,  4.73it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 3/14 [00:00<00:03,  3.64it/s]\u001b[A\n",
      " 29%|████████████▌                               | 4/14 [00:01<00:03,  3.14it/s]\u001b[A\n",
      " 36%|███████████████▋                            | 5/14 [00:01<00:03,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 6/14 [00:02<00:02,  2.69it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 7/14 [00:02<00:02,  2.58it/s]\u001b[A\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:02<00:02,  2.51it/s]\u001b[A\n",
      " 64%|████████████████████████████▎               | 9/14 [00:03<00:02,  2.46it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:03<00:01,  2.43it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:04<00:01,  2.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:04<00:00,  2.40it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:05<00:00,  2.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.45996522903442383, 'eval_accuracy_score': 0.8283582089552238, 'eval_precision': 0.5369127516778524, 'eval_recall': 0.6299212598425197, 'eval_f1': 0.5797101449275364, 'eval_runtime': 6.2771, 'eval_samples_per_second': 34.889, 'epoch': 2.0}\n",
      " 40%|████████████████▊                         | 62/155 [01:34<01:57,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.40it/s]\u001b[A\n",
      " 60%|█████████████████████████▏                | 93/155 [02:14<01:18,  1.26s/it][INFO|trainer.py:1600] 2021-02-09 17:15:53,611 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:15:53,611 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:15:53,611 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████▎                                     | 2/14 [00:00<00:02,  4.75it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 3/14 [00:00<00:03,  3.66it/s]\u001b[A\n",
      " 29%|████████████▌                               | 4/14 [00:01<00:03,  3.14it/s]\u001b[A\n",
      " 36%|███████████████▋                            | 5/14 [00:01<00:03,  2.87it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 6/14 [00:02<00:02,  2.69it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 7/14 [00:02<00:02,  2.58it/s]\u001b[A\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:02<00:02,  2.51it/s]\u001b[A\n",
      " 64%|████████████████████████████▎               | 9/14 [00:03<00:02,  2.47it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:03<00:01,  2.44it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:04<00:01,  2.42it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:04<00:00,  2.40it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.43037164211273193, 'eval_accuracy_score': 0.8333333333333334, 'eval_precision': 0.5759717314487632, 'eval_recall': 0.6417322834645669, 'eval_f1': 0.6070763500931099, 'eval_runtime': 6.2839, 'eval_samples_per_second': 34.851, 'epoch': 3.0}\n",
      " 60%|█████████████████████████▏                | 93/155 [02:20<01:18,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      " 80%|████████████████████████████████▊        | 124/155 [03:00<00:39,  1.27s/it][INFO|trainer.py:1600] 2021-02-09 17:16:39,906 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:16:39,906 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:16:39,906 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████▎                                     | 2/14 [00:00<00:02,  4.74it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 3/14 [00:00<00:03,  3.64it/s]\u001b[A\n",
      " 29%|████████████▌                               | 4/14 [00:01<00:03,  3.12it/s]\u001b[A\n",
      " 36%|███████████████▋                            | 5/14 [00:01<00:03,  2.85it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 6/14 [00:02<00:02,  2.68it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 7/14 [00:02<00:02,  2.58it/s]\u001b[A\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:02<00:02,  2.51it/s]\u001b[A\n",
      " 64%|████████████████████████████▎               | 9/14 [00:03<00:02,  2.47it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:03<00:01,  2.44it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:04<00:01,  2.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:04<00:00,  2.40it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.37149402499198914, 'eval_accuracy_score': 0.8532338308457711, 'eval_precision': 0.6816479400749064, 'eval_recall': 0.7165354330708661, 'eval_f1': 0.6986564299424184, 'eval_runtime': 6.304, 'eval_samples_per_second': 34.74, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████▊        | 124/155 [03:06<00:39,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 155/155 [03:46<00:00,  1.27s/it][INFO|trainer.py:1600] 2021-02-09 17:17:26,401 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:17:26,401 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:17:26,401 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████▎                                     | 2/14 [00:00<00:02,  4.72it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 3/14 [00:00<00:03,  3.63it/s]\u001b[A\n",
      " 29%|████████████▌                               | 4/14 [00:01<00:03,  3.12it/s]\u001b[A\n",
      " 36%|███████████████▋                            | 5/14 [00:01<00:03,  2.84it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 6/14 [00:02<00:02,  2.68it/s]\u001b[A\n",
      " 50%|██████████████████████                      | 7/14 [00:02<00:02,  2.58it/s]\u001b[A\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:02<00:02,  2.52it/s]\u001b[A\n",
      " 64%|████████████████████████████▎               | 9/14 [00:03<00:02,  2.47it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:03<00:01,  2.43it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:04<00:01,  2.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:04<00:00,  2.40it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.338486909866333, 'eval_accuracy_score': 0.8694029850746269, 'eval_precision': 0.68, 'eval_recall': 0.7362204724409449, 'eval_f1': 0.7069943289224954, 'eval_runtime': 6.3012, 'eval_samples_per_second': 34.755, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 155/155 [03:53<00:00,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1007] 2021-02-09 17:17:32,703 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 233.2114, 'train_samples_per_second': 0.665, 'epoch': 5.0}    \n",
      "100%|█████████████████████████████████████████| 155/155 [03:53<00:00,  1.50s/it]\n",
      "[INFO|trainer.py:1408] 2021-02-09 17:17:32,708 >> Saving model checkpoint to ./model\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 17:17:32,711 >> Configuration saved in ./model/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 17:17:35,301 >> Model weights saved in ./model/pytorch_model.bin\n",
      "02/09/2021 17:17:35 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1600] 2021-02-09 17:17:35,306 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:17:35,306 >>   Num examples = 219\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:17:35,306 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.38it/s]\n",
      "02/09/2021 17:17:41 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_loss = 0.338486909866333\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_accuracy_score = 0.8694029850746269\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_precision = 0.68\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_recall = 0.7362204724409449\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_f1 = 0.7069943289224954\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_runtime = 6.3107\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     eval_samples_per_second = 34.703\n",
      "02/09/2021 17:17:41 - INFO - __main__ -     epoch = 5.0\n",
      "02/09/2021 17:17:41 - INFO - filelock -   Lock 139632445021152 acquired on ./data/cached_test_XLMRobertaTokenizer_128.lock\n",
      "02/09/2021 17:17:41 - INFO - utils_ner -   Loading features from cached file ./data/cached_test_XLMRobertaTokenizer_128\n",
      "02/09/2021 17:17:41 - INFO - filelock -   Lock 139632445021152 released on ./data/cached_test_XLMRobertaTokenizer_128.lock\n",
      "[INFO|trainer.py:1600] 2021-02-09 17:17:41,635 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:1601] 2021-02-09 17:17:41,635 >>   Num examples = 547\n",
      "[INFO|trainer.py:1602] 2021-02-09 17:17:41,635 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████████| 35/35 [00:14<00:00,  2.40it/s]02/09/2021 17:17:58 - INFO - __main__ -     eval_loss = 0.3610181510448456\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_accuracy_score = 0.8699596774193549\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_precision = 0.680786686838124\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_recall = 0.72\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_f1 = 0.6998444790046656\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_runtime = 15.7547\n",
      "02/09/2021 17:17:58 - INFO - __main__ -     eval_samples_per_second = 34.72\n",
      "100%|███████████████████████████████████████████| 35/35 [00:16<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_ner.py \\\n",
    "    --model_name_or_path \"xlm-roberta-large\" \\\n",
    "    --labels ./labels.txt \\\n",
    "    --data_dir ./data/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir ./model \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --adafactor \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 0.00005 \\\n",
    "    --warmup_steps 20 \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --seed 1 \\\n",
    "    --num_train_epochs 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
